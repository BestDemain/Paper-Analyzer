{
  "标题": "[章节内容长度: 37字符。MapCoder: Multi-Agent Code Generation]",
  "摘要": "[章节内容长度: 1418字符。Code synthesis, which requires a deep under-\nstanding of complex natural language (NL)\nproblem descriptions, generation of code in-\nstructions for complex algorithms and data\nstructures, and the successful execution of com-\nprehensive unit tests, presents a significant\nchallenge. Thus, while large language mod-\nels (LLMs) demonstrate impressive proficiency\nin natural language processing (NLP), their\nperformance in code generation tasks remains\nlimited.\nIn this paper, we introduce a new\napproach to code generation tasks leveraging\nthe multi-agent prompting that uniquely repli-\ncates the full cycle of program synthesis as\nobserved in human developers. Our framework,\nMapCoder, consists of four LLM agents specif-\nically designed to emulate the stages of this\ncycle: recalling relevant examples, planning,\ncode generation, and debugging. After con-\nducting thorough experiments, with multiple\nLLMs ablations and analyses across eight chal-\nlenging competitive problem-solving and pro-\ngram synthesis benchmarks—MapCoder{v0}\ncases remarkable code generation capabil-\nities,\nachieving their new state-of-the-art\n(pass@1) results—(HumanEval{v1}, MBPP\n83.1%, APPS{v2}, CodeContests{v3},\nand xCodeEval{v4}).\nMoreover, our\nmethod consistently delivers superior perfor-\nmance across various programming languages\nand varying problem difficulties. We open-\nsource our framework at{v5}\ncom/Md-Ashraful-Pramanik/MapCoder.\n1]",
  "第1章 Introduction": "[章节内容长度: 6197字符。Introduction\nComputer Programming has emerged as an ubiq-\nuitous problem-solving tool that brings tremen-\ndous benefits to every aspects of our life (Li et al.,\n2022a;{v6},{v7};{v8},{v9}).\nTo\nmaximize programmers’ productivity, and enhance\naccessibility, automation in program synthesis is\nparamount. With the growth of LLMs, significant\nadvancements have been made in program synthe-\nsis—driving us in an era where we can generate\nfully executable code, requiring no human inter-\nvention (Chowdhery et al.,{v10};{v11},\n2022).\nDespite LLMs’ initial success and the scaling\nup of model size and data, many of these models\nstill struggle to perform well on complex problem-\nsolving tasks, especially in competitive program-\nming problems (Austin et al.,{v12}). To mitigate\nthis gap, in this paper, we introduce{v13}: a\nMulti-Agent{v14}rompting Based Code Generation\napproach that can seamlessly synthesize solutions\nfor competition-level programming problems.\nCompetitive programming or competition-level\ncode generation, often regarded as the pinnacle\nof problem-solving, is an challenging task. It re-\nquires a deep comprehension of NL problem de-\nscriptions, multi-step complex reasoning beyond\nmere memorization, excellence in algorithms and\ndata structures, and the capability to generate sub-\nstantial code that produces desired outputs aligned\nwith comprehensive test cases (Khan et al.,{v15}).\nEarly approaches utilizing LLMs for code gener-\nation employ a direct prompting approach, where\nLLMs generate code directly from problem descrip-\ntions and sample I/O (Chen et al.,{v16}). Recent\nmethods like chain-of-thought (Wei et al.,{v17})\nadvocates modular or pseudo code-based genera-\ntion to enhance planning and reduce errors, while\nretrieval-based approaches such as{v18}\n(2021) leverage relevant problems and solutions to\nguide LLMs’ code generations. However, gains in\nsuch approaches remains limited in such a complex\ntask like code generation where LLMs’ generated\ncode often fails to pass the test cases and they do\nnot feature bug-fixing schema (Ridnik et al.,{v19}).\nA promising solution to the above challenge\nis self-reflection (Shinn et al.,{v20};{v21},\n2022), which iteratively evaluates the generated\ncode against test cases, reflects on mistakes and\n1\narXiv:2405.11403v1  [cs.CL]  18 May 2024\n\n\nDynamic Traversal Block\n(Elaborated View)\nFailed in\nsample I/O\nFailed in sample I/O\nmax turn done\nback to planner agent\nProblem\nWrite a python code for\ndescending order sorting.\nThe program will take an\narray and return a new\narray \nsorted \nin \nthe\n{v22}\n[3,2,1]\nPlan 2\nPlan 1\nPlan k\n0.9\n0.8\n0.7\n0.7\n0.9\n0.8\n0.7\n0.9\n0.8\nCode\nModified Code\nBugs Fixed\nPassed in\nsample I/O\nIterative Tests\nand debug, max\n{v23}\nDynamic Traversal Block\nFull Pipeline View\nk relevant examples\n(problem, plan, code)\nGenerated Code\nSorted Plans\nPlan with Confidence score\nRetrieval\nAgent\nPlanning\nAgent\nCoding\nAgent\nDebugging\nAgent\n//\nBug fixed and\nFinal Code\nProblem\nWrite a python code for\ndescending order\nsorting...\nk plans for\noriginal problem\nFigure 1: Overview of{v24}\nby planning, coding, and iterative debugging agents. Our dynamic traversal (bottom) considers the confidence of the\ngenerated plans as their reward scores and leverages them to guide the code generation accordingly.\nmodifies accordingly. However, such approaches\nhave limitations too. Firstly, while previous stud-\nies indicate that superior problem-solving capa-\nbilities are attained when using in-context exem-\nplars (Shum et al.,{v25};{v26},{v27};{v28}\net al.,{v29}) or plans (Jiang et al.,{v30}), these\napproaches, during both code generation and de-\nbugging, only leverage the problem description\nitself in a zero-shot manner. Consequently, their\ngains can be limited.\nTo confront the above challenge, we develop\nMapCoder{v31}\nwith possible auxiliary supervision. We draw inspi-\nration from human programmers, and how they use\nvarious signals/feedback while programming. The\nhuman problem-solving cycle involves recalling\npast solutions, planning, code writing, and debug-\nging.{v32}\nagents - retrieval, planning, coding, and debug-\nging. In contrast to relying on human annotated\nexamples, or external code retrieval models, we\nempower our retrieval agent to autonomously re-\ntrieve relevant problems itself (Yasunaga et al.,\n2023). Moreover, we design a novel structured\npipeline schema that intelligently cascades the\nLLM agents and incorporates a dynamic iteration\nprotocol to enhance the generation procedure at\nevery step. Figure{v33}\napproach,{v34}\nAdditionally, existing iterative self-reflection\nmethods rely on extra test cases generated by LLM\nagents (e.g., AgentCoder (Huang et al.,{v35}),\nLATS (Zhou et al.,{v36}), self-reflection (Shinn\net al.,{v37})) or external tools, compounding the\nchallenges. Test case generation is equally chal-\nlenging as code generation (Pacheco et al.,{v38}),\nand incorrect test cases can lead to erroneous code.\nBlindly editing code based on these test cases can\nundermine problem-solving capabilities. For in-\nstance, while self-reflection boosts GPT-4’s perfor-\nmance on the HumanEval dataset, it drops by 3%\non the MBPP dataset (Shinn et al.,{v39}). Upon\nidentification, to validate this, on the HumanEval\ndataset itself, we replace their GPT-4 with Chat-\nGPT, and see that model performance drops by\n26.3%. Therefore, our debugging agent performs\nunit tests and bug fixing using only the sample I/O,\nwithout any artifact-more plausible for real-world\nwidespread adoption.\nWe evaluate{v40}\ngramming synthesis benchmarks including both\nbasic programming like HumanEval, MBPP and\nchallenging competitive program-solving bench-\nmarks like APPS, CodeContests and xCodeEval.\nWith multiple different LLMs including ChatGPT,\nGPT-4, and Gemini Pro, our approach significantly\nenhances their problem-solving capabilities - con-\nsistently achieving new SOTA performances, out-\nperforming strong baselines like Reflexion (Shinn\net al.,{v41}), and AlphaCodium (Ridnik et al.,\n2024). Moreover, our method consistently delivers\nsuperior performance across various programming\nlanguages and varying problem difficulties. Fur-\nthermore, with detailed ablation studies, we ana-\nlyze{v42}\n2\n\n\n2]",
  "第2章 Related Work": "[章节内容长度: 2063字符。Program Synthesis:{v43}\nlong standing history in AI systems (Manna and\nWaldinger,{v44}). A large number of prior research\nattempted to address it via search/data flow ap-\nproaches (Li et al.,{v45};{v46}\ndinov,{v47};{v48},{v49};{v50},\n2011). LMs, prior to LLMs, attempt to generate\ncode by fine-tuning (i.e., training) neural language\nmodels (Wang et al.,{v51};{v52},{v53};\nFeng et al.,{v54};{v55},{v56};{v57}\nNeubig,{v58};{v59},{v60};\nRabinovich et al.,{v61};{v62},{v63}), con-\nversational intents or data flow features (Andreas\net al.,{v64};{v65},{v66}).\nLarge Language Models:{v67}\nbeen developed for Code synthesis (Li et al.,{v68};\nFried et al.,{v69};{v70},{v71};{v72},\n2021;{v73},{v74};{v75},{v76}). Re-\ncent open source LLMs include Llama-2 (Touvron\net al.,{v77}), CodeLlama-2 (Roziere et al.,{v78}),\nMistral (Jiang et al.,{v79}) Deepseek Coder (Guo\net al.,{v80}), MoTCoder (Li et al.,{v81}) that are\ncapable of solving many basic programming tasks.\nApproach\nSelf-retrieval \nPlanning  \nAdditional \ntest cases \ngeneration\nDebugging\nReflexion\n✗\n✗\n✔\n✔\nSelf-planning\n✗\n✔\n✗\n✗\nAnalogical\n✔\n✔\n✗\n✗\nAlphaCodium\n✗\n✗\n✔\n✔\nMapCoder\n✔\n✔\n✗\n✔\nTable 1: Features in code generation prompt techniques.\nPrompting LLMs:{v82},\nLLM prompting can be summarized into three cat-\negories: retrieval (Yasunaga et al.,{v83};{v84}\net al.,{v85},{v86}); planning (Wei et al.,{v87};\nJiang et al.,{v88}); debugging (Ridnik et al.,{v89};\nChen et al.,{v90},{v91};{v92},{v93}) apart from\nthe direct code generation approaches. In con-\ntrast, we combine all these paradigms and bridge\ntheir gaps (See Table{v94}). Among others, in differ-\nent contexts of generic problem-solving, Tree-of-\nthoughts (Yao et al.,{v95}), and Cumulative rea-\nsoning (Zhang et al.,{v96}) approaches consider\na tree traversal approach to explore different sub-\nsteps towards a solution while our code generation\napproach mirrors the human programming cycle\nthrough various LLM agents. Notably, our traver-\nsal does not rely on sub-steps toward the solution\nbut instead utilizes different forms of complete so-\nlutions.]",
  "第3章 MapCoder": "[章节内容长度: 6469字符。MapCoder\nOur goal is to develop a multi-agent code genera-\ntion approach for competitive problem-solving. In\norder to do so, our framework,{v97}, repli-\ncates the human programming cycle through four\nLLM agents - retrieval, plan, code, and debug. We\ndevise a pipeline sequence for{v98}, intelli-\ngently cascading the agents in a structured way and\nenhancing each agent’s capability by augmenting\nin-context learning signals from previous agents\nin the pipeline.\nHowever, not all the agent re-\nsponses/outputs are equally useful. Therefore, ad-\nditionally,{v99}\ntraversal schema to interact among corresponding\nagents dynamically, iteratively enhancing the gener-\nated code by, for example, fixing bugs, while maxi-\nmizing the usage of the LLM agents. In this section,\nwe first discuss the agents (as per the pipeline), their\nprompts, and interactions, followed by the dynamic\nagent traversal protocol in{v100}\ngeneration for competitive problem-solving.\n3.1\nRetrieval Agent\nOur first agent, the{v101}, recalls past\nrelevant problem-solving instances, akin to human\nmemory. It finds{v102}\nlems without manual crafting or external retrieval\nmodels. Instead, we leverage the LLM agent it-\nself, instructing it to generate such problems. Our\nprompt extends the analogical prompting princi-\nples (Yasunaga et al.,{v103}), generating examples\nand their solutions simultaneously, along with ad-\nditional metadata (e.g., problem description, code,\nand plan) to provide the following agents as aux-\niliary data. We adopt a specific sequence of in-\nstructions, which is crucial for the prompt’s ef-\nfectiveness.\nIn particular, initially, we instruct\nthe LLM to produce similar and distinct problems\nand their solutions, facilitating problem planning\nreverse-engineering. Then, we prompt the LLM\nto generate solution code step-by-step, allowing\npost-processing to form the corresponding plan.\nFinally, we direct the LLM to generate relevant\nalgorithms and provide instructional tutorials, en-\nabling the agent to reflect on underlying algorithms\nand generate algorithmically similar examples.\n3.2\nPlanning Agent\nThe second agent, the{v104}, aims to cre-\nate a step-by-step plan for the original problem.\nOur{v105}\n3\n\n\n \n \nPlanning Generation Prompt: \nGiven a competitive programming problem generate a concrete planning to \nsolve the problem. \n# Problem: {Description of a self-retrieved example problem} \n# Planning: {Planning of that problem} \n## Relevant Algorithm to solve the next problem:  \n{Algorithm retrieved by the Retrieval Agent} \n## Problem to be solved: {Original Problem} \n## Sample Input/Outputs: {Sample I/Os} \n \nConfidence Generation Prompt: \nGiven a competitive programming problem and a plan to solve the problem \nin {language} tell whether the plan is correct to solve this problem. \n \n# Problem: {Original Problem} \n# Planning: {Planning of our problem from previous step} \n \n Planning Agent \nFigure 2: Prompt for{v106}.\nobtained from the retrieval agent to generate plans\nfor the original problem. A straightforward ap-\nproach would be to utilize all examples collectively\nto generate a single target plan. However, not all re-\ntrieved examples hold equal utility. Concatenating\nexamples in a random order may compromise the\nLLM’s ability to generate accurate planning. For\ninstance,{v107}2023) demonstrated that even\nrepeating more relevant information (e.g., query)\ntowards the end of the in-context input aids LLM\nreasoning more effectively than including relatively\nless relevant contexts. A similar conclusion of \"sep-\narating noisy in-context data\" can also be drawn\nfrom the state-of-the-art retrieval augmented gen-\neration approaches like{v108}2023). There-\nfore, we generate a distinct target plan for each\nretrieved example. Additionally, multiple plans\noffer diverse pathways to success.\nTo help the generation steps in the following\nagents with the utility information for each plan,\nour designed prompt for the planning agent asks\nthe LLM to generate both plans and a confidence\nscore. Figure{v109}\n3.3\nCoding Agent\nNext is the{v110}. It takes the problem de-\nscription, and a plan from the{v111}\nput and translates the corresponding planning into\ncode to solve the problem. During the traversing\nof agents,{v112}\nand one particular plan from the{v113},\ngenerates the code, and test on sample I/O. If the\ninitial code fails, the agent transfers it to the next\nagent for debugging. Otherwise, predicts that as\nthe final solution.\n3.4\nDebugging Agent\nFinally, the{v114}\nfrom the problem description to rectify bugs in the\ngenerated code. Similar to humans cross-checking\ntheir plan while fixing bugs, our pipeline supple-\nments the{v115}\n{v116}. This plan-derived debugging sig-\nnificantly enhances bug fixing in{v117}, under-\nscoring the pivotal roles played by both the{v118}\n{v119}\neration process. We verify this in Section{v120}. For\neach plan, this process is repeated{v121}\nprompt for this step is illustrated in Figure{v122}. Note\nthat, different from Reflexion (Shinn et al.,{v123})\nand AlphaCodium (Ridnik et al.,{v124}), our{v125}\n{v126}\ncase generation in the pipeline.\n \n \nGiven a competitive programming problem you have generated {language} \ncode to solve the problem. But the generated code can not pass sample \ntest cases. Improve your code to solve the problem correctly. \n \n## Relevant Algorithm to solve the next problem:{v127}\n{Algorithm retrieved by Retrieval Agent} \n## Planning:{v128}\n## Code:{v129}\n## Modified Planning: \n## Let's think step by step to modify {language} Code for solving \nthis problem. \nDebugging Agent \nFigure 3: Prompt for{v130}.\n3.5\nDynamic Agent Traversal\nThe dynamic traversal in{v131}\n{v132}, which outputs the plans for the\noriginal problem with confidence scores. These\nplans are sorted, and the highest-scoring one is sent\nto the Coding Agent. The Coding Agent translates\nthe plan into code, tested with sample I/Os. If all\npass, the code is returned; otherwise, it’s passed to\n{v133}. They attempt to rectify the code\niteratively up to{v134}\nreturned; otherwise, responsibility shifts back to\nthe{v135}\nplan. This iterative process continues for{v136}\ntions, reflecting a programmer’s approach. We sum-\nmarize our agent traversal in Algorithm{v137}\npendix. Our algorithm’s complexity is{v138}({v139}). An\nexample illustrating{v140}’s problem-solving\ncompared to Direct, Chain-of-thought, and Reflex-\nion approaches is in Figure{v141}. All detailed prompts\nfor each agent are in Appendix{v142}.]",
  "第4章 Experimental Setup": "[章节内容长度: 4721字符。Experimental Setup\n4.1\nDatasets\nFor extensive evaluation, we have used eight bench-\nmark datasets: five from basic programming and\nthree from complex competitive programming do-\nmains.\nFive basic programming datasets are:\n4\n\n\n \n \ndef sum_squares(lst): \n{v143}\n{v144}\n{v145}\n{v146}\n{v147}\n{v148}\n{v149}\n{v150}\n{v151}\n{v152} \n \nProblem: \n \ndef sum_squares(lst): \n{v153}total {v154}0 \n{v155}for num in lst: \n{v156}total {v157}int(num) ** 2) \n{v158}return total \n \nFailed Case: {v159}\nExplanation: Here, the int function truncates the floating \npoint number, making it a lower integer. However, \naccording to the problem, the number should be an upper \ninteger.  \nDirect \ndef sum_squares(lst): \n{v160}return sum((int{v161})**2 for x{v162}lst) \n \n \ndef sum_squares(lst): \n{v163}return sum(int({v164})**2 for x{v165}lst) \n \n \ndef sum_squares(lst): \n{v166}return sum(round(x)**2 for x in lst) \n \n \n \nFailed Case: {v167}\nExplanation: This implementation rounds the given number but the \nproblem requirement is ceiling. Reflexion failed to improve the code \nbecause it focused on self-generated test case {v168}\n{v169}instead of considering all the sample I/O{v170}For \nthis particular test case, rounding also works fine thus ending up \nwith a wrong implementation. \nReflexion \n \nRetrieval Agent: {v171}\n{v172}\n{v173} \nExample 1:{v174}\n...  \n \nPlanning Agent: {v175}\nTo solve this problem, we can iterate through the list, \nround each number to the upper integer, square it, and \nthen add it to a running total. Finally, we return the \ntotal sum of all squared numbers in the list. \n \nCoding Agent: {v176} \nimport math \ndef sum_squares(lst): \n{v177}total {v178}0 \n{v179}for num in lst: \n{v180}total {v181}int(num) ** 2) \n{v182}return total \nTesting with all the Sample I/O.  \nBut it failed in the following sample I/O: \n{v183} \n \nDebugging Agent: {v184}\n{v185}\nimport math \ndef sum_squares(lst): \n{v186}total {v187}0 \n{v188}for num in lst: \n{v189}total {v190}math.ceil(num) ** 2) \n{v191}return total \n \nAll sample input-output pairs now passed.  \nThe code is evaluated against private test cases, and  \nit passed all of them as well. \nMapCoder \n \ndef sum_squares(lst): \n{v192}total {v193}0 \n{v194}for num in lst: \n{v195}total {v196}int(num{v197}if {v198}else 0)) ** 2 \n{v199}return total \nFailed Case: assert{v200}\nExplanation: After applying the int function it adds 1 if the number \nis a floating point that will not work for negative floating-point \nnumbers.  \nCoT \nSample I/O \nDeviation from Planning \nFix according to plan \nFigure 4: Example problem and solution generation using Direct, CoT, Reflexion, and{v201}\nMapCoder{v202}\nfixing.\nHumanEval{v203}Chen et al.,{v204}),{v205}\nET{v206}Dong et al.,{v207}),{v208}Liu et al.,\n2023),{v209}) (Austin et al.,{v210}), and{v211}\nET{v212}Dong et al.,{v213}). HumanEval-ET, EvalPlus\nextend HumanEval and MBPP-ET comprehends\nMBPP by incorporating more test cases. The prob-\nlem set size of HumanEval and MBPP (and their\nextensions) are 164 and 397, respectively. Due to\nthe absence of sample I/O in MBPP and MBPP-\nET, our approach for code moderation involves\nrandomly removing one test-case from MBPP-ET\nfor each problem and provide this test-case as a\nsample I/O for the problem. Importantly, this re-\nmoved test-case is carefully selected to ensure mu-\ntual exclusivity from the hidden test sets in MBPP\nand MBPP-ET. Three competitive programming\ndatasets are: Automated Programming Progress\nStandard (APPS),{v214}Khan et al.,{v215}),\nand{v216}, where we have used 150, 106,\nand 156 problems, respectively, in our experiments.\n4.2\nBaselines\nWe have compared{v217}\nlines and state-of-the-art approaches.\nDirect\nPrompting instructs language models to generate\ncode without explicit guidance, relying on their\ninherent capabilities of LLM. Chain of Thought\nPrompting (CoT) (Wei et al.,{v218}) breaks down\nproblems into step-by-step solutions, enabling ef-\nfective tackling of complex tasks.{v219}\nPrompting (Jiang et al.,{v220}) divides the code\ngeneration task into planning and implementation\nphases.{v221}Ya-\nsunaga et al.,{v222}) instructs models to recall rel-\nevant problems from training data.\nReflexion\n(Shinn et al.,{v223}) provides verbal feedback to\nenhance solutions based on unit test results.{v224}\ncollaboration{v225}Dong et al.,{v226}) proposes a\nframework where different LLMs act as analyst,\ncoder, and tester to cooperatively generate code\nfor complex tasks, achieving better performance\nthan directly using a single LLM.{v227}\n(Ridnik et al.,{v228}) iteratively refines code based\non AI-generated input-output tests.\n4.3\nFoundation Models, Evaluation Metric,{v229},\nand{v230}\nWith{v231}\nothers, we evaluate all the datasets using{v232}\n(gpt-3.5-turbo-1106),{v233}]",
  "第5章 Simple Problems": "[章节内容长度: 1726字符。Simple Problems\nContest-Level Problems\nLLM\nApproach\nHumanEval HumanEval\nET\nEvalPlus\nMBPP\nMBPP\nET\nAPPS\nxCodeEval CodeContest\nChatGPT\nDirect\n{v234}\n{v235}\n{v236}\n{v237}\n{v238}\n{v239}\n{v240}\n{v241}\nCoT\n{v242}\n{v243}\n{v244}\n{v245}\n{v246}\n{v247}\n{v248}\n{v249}\nSelf-Planning\n{v250}\n{v251}\n{v252}\n{v253}\n{v254}\n{v255}\n{v256}\n{v257}\nAnalogical\n{v258}\n{v259}\n{v260}\n{v261}\n{v262}\n{v263}\n{v264}\n{v265}\nReflexion\n{v266}\n{v267}\n{v268}\n{v269}\n{v270}\n{v271}\n{v272}\n{v273}\nSelf-collaboration\n{v274}\n{v275}\n{v276}\n{v277}\n{v278}\n{v279}\n{v280}\n{v281}\nMapCoder\n{v282}\n{v283}\n{v284}\n{v285}\n{v286}\n{v287}\n{v288}\n{v289}\n{v290}\n{v291}\n{v292}\n{v293}\n{v294}\n{v295}\n{v296}\n{v297}\nGPT4\nDirect\n{v298}\n{v299}\n{v300}\n{v301}\n{v302}\n{v303}\n{v304}\n{v305}\nCoT\n{v306}\n{v307}\n{v308}\n{v309}\n{v310}\n{v311}\n{v312}\n{v313}\nSelf-Planning\n{v314}\n{v315}\n{v316}\n{v317}\n{v318}\n{v319}\n{v320}\n{v321}\nAnalogical\n{v322}\n{v323}\n{v324}\n{v325}\n{v326}\n{v327}\n{v328}\n{v329}\nReflexion\n{v330}\n{v331}\n{v332}\n{v333}\n{v334}\n{v335}\n{v336}\n{v337}\nMapCoder\n{v338}\n{v339}\n{v340}\n{v341}\n{v342} \n{v343}\n{v344}\n{v345}\n{v346}\n{v347}\n22.0% \n{v348}\n{v349} \n{v350}\n{v351} \n{v352}\nTable 2: Pass@1 results for different approaches. The results of the yellow and blue colored cells are obtained from\nJiang et al.{v353}2023b) and{v354}2023), respectively. The results of the Self-collaboration{v355}2023b)\npaper are collected from their paper. The green texts indicate the state-of-the-art results, and the red text is gain over\nDirect Prompting approach.\nfrom OpenAI and{v356}\nhave also evaluated our method using an open-\nsource LLM, Mistral-7B-instruct. We have used\nthe Pass@k evaluation metric, where the model\nis considered successful if at least one of the{v357}\ngenerated solutions is correct.]",
  "第6章 Results": "[章节内容长度: 4707字符。Results\nIn this section, we evaluate the code generation\ncapabilities of our framework,{v358}, for com-\npetitive problem solving. Our experimental results\nare reported in Table{v359}. Overall,{v360}\na tremendous excellence in code generation, sig-\nnificantly outperforms all baselines, and achieves\nnew state-of-the-art results in all benchmarks. In\ngeneral the scales with GPT-4 are higher than Chat-\nGPT.\n5.1\nPerformance on basic code generation\nThe highest scale of performance (Pass@1) scores\nare observed in simple program synthesis tasks like\nHumanEval, MBPP in Table{v361}. Though with the\nsimpler problem (non-contests) datasets such as\nHumanEval, HumanEval-ET, the current state-of-\nthe-art method, Reflexion (Shinn et al.,{v362}) per-\nform reasonably well, this approach does not gener-\nalize across varying datasets depicting a wide vari-\nety of problems. Self-reflection techniques enhance\nGPT-4’s performance on HumanEval but result in a\n3% decrease on the MBPP dataset. Similarly, with\nChatGPT, there’s a notable 26.3% drop in perfor-\nmance where in several cases their AI generated\ntest cases are incorrect. We observe that 8% of fail-\nures in HumanEval and 15% in MBPP is caused by\ntheir AI generates incorrect test cases while our ap-\nproach is independent of AI test cases, and consis-\ntently improves code generations in general. Con-\nsequently, even in HumanEval, with GPT-4, our\nPass@1 surpasses Reflexion by{v363}3%. On top, in all\nfour simple programming datasets,{v364}\nhances the Direct prompting significantly with a\nmaximum of 88% on HumanEvalET by ChatGPT.\n5.2\nPerformance on competitive problem\nsolving\nThe significance of{v365}\nclearly when evaluated in competitive problem-\nsolving contexts. Across datasets such as APPS,\nxCodeEval, and CodeContests,{v366}\nstrates substantial enhancements over Direct\nprompting methods, with improvements of 41.3%,\n52.6%, and 132.8% for ChatGPT, and 73.7%,\n41.2%, and 135.1% for GPT4, respectively. No-\ntably, the most challenging datasets are APPS\nand CodeContest, where{v367}’s performance\nstands out prominently.\nWe deliberately com-\n6\n\n\nFigure 5: The number of correct answers wrt algorithm types (tags) and difficulty levels (xCodeEval dataset).\npare against strong baselines on these datasets, re-\ngardless of whether they are prompt-based or not.\nImportantly, on CodeContest our Pass@1 results\nmatch the Pass@5 scores of the concurrent state-of-\nthe-art model AlphaCodium (Ridnik et al.,{v368}):\n28.5% vs. their 29% (see Table{v369}). Furthermore,\nour Pass@5 results demonstrate an additional im-\nprovement of 12.8%. On APPS,{v370}\ntently surpasses the Pass@1 scores of all baseline\nprompts for both ChatGPT and GPT-4.\nCodeContest (Pass@5)\nApproach\nChatGPT\nGPT4\nDirect\n11.2%\n18.8%\nAlphaCodium\n17.0%\n29.0%\nMapCoder\n18.2% {v371}\n35.2% {v372}\nTable 3: Pass@5 results on CodeContest dataset. Alph-\nCodium result are from{v373}2024). The green\ncells indicate the SoTA and the red text indicates im-\nprovement w.r.t Direct approach.\n5.3\nPerformance with Varying Difficulty\nLevels\nThe APPS dataset comprises problems categorized\ninto three difficulty levels: (i) Introductory, (ii) In-\nterview, and (iii) Competition. Figure{v374}\nthe performance of various competitive approaches\nfor these three categories. The results reveal that\nour{v375}\nwith highest gain in competitive problem-solving\nindicating its superior code generation capabilities\nin general, and on top, remarkable effectiveness\nin competitive problem-solving. In order to gather\nmore understanding on what algorithm problems\nit’s capable of solving and in fact much difficulty\nlevel it can solve, we have also conducted a compar-\nison between{v376}\nconsidering the difficulty levels1 and tags2 present\nin the xCodeEval dataset. The results of this com-\nparison are depicted in Figure{v377}. This comparison\nshowcases that{v378}\nous algorithm types and exhibits superior perfor-\nmance even in higher difficulty levels, compared to\nthe Direct approach. However, beyond (mid-level:\n{v379}\nFigure 6: Performance vs problem types (APPS).\n5.4\nPerformance Across Different LLMs\nTo show the robustness of{v380}\nous LLMs, we evaluate{v381}\nPro, a different family of SoTA LLM in Table{v382}.\nWe also evaluate{v383}\nLLM Mistral-7B instruct in Table{v384}. As expected,\nour method shows performance gains over other\nbaseline approaches in equitable trends on both\nsimple (HumanEval) and contest-level problems\n(CodeContest).\n1Difficulty levels in xCodeEval dataset represents an inte-\nger number, a higher value means more difficult problem\n2Tags in xCodeEval dataset represents algorithm type that\ncan be used to solve the problem i.e., greedy, dp, brute-force,\nconstructive, and so on.]",
  "第7章 LLM": "[章节内容长度: 837字符。LLM\nApproach\nHumanEval\nCodeContest\nGemini\nDirect\n64.6%\n3.6%\nCoT\n66.5%\n4.8%\nMapCoder\n69.5% {v385}\n4.8% {v386}\nTable 4: Pass@1 results with using Gemini Pro. The\nred text is gain over Direct Prompting approach.\nLLM\nApproach\nHumanEval\nHumanEval-ET\nMistral\nDirect\n27.3%\n27.3%\nCoT\n45.5%%\n42.4%\nMapCoder\n57.6% {v387}\n48.5% {v388}\nTable 5: Pass@1 results with using Mistral-7B-instruct.\nThe red text is gain over Direct Prompting approach.\n5.5\nPerformance Across Different\nProgramming Languages\nFurthermore, we evaluate model performances us-\ning{v389}\nguages. We utilize the xCodeEval dataset, which\nfeatures multiple languages. Figure{v390}\nconsistent proficiency across different program-\nming languages is achieved by{v391}\nspect to baselines.\nFigure 7: The number of correct answers wrt different\nprogramming languages (xCodeEval dataset).]",
  "第8章 Ablations Studies and Analyses": "[章节内容长度: 1930字符。Ablations Studies and Analyses\nWe present the ablation study of the{v392}\nHumanEval dataset as the problems are simpler\nand easy to diagnose by us humans.\n6.1\nImpact of Different Agents\nWe have also conducted a study by excluding cer-\ntain agents from our{v393}, which helps us in-\nvestigate each agent’s impact in our whole pipeline.\nAs expected, the results (Table{v394}) show that every\nagent has its role in the pipeline as turning off any\nagent decreases the performance of{v395}. Fur-\nthermore, we observe that the Debugging Agent\nhas the most significant impact on the pipeline, as\nevidenced by a performance drop of 17.5% when\nexcluding this agent exclusively, and an avg perfor-\nmance drop of 24.83% in all cases. The{v396}\n{v397}\nof 16.7% in all cases. In Table{v398}), we perform an\nablation study of our multi-agent framework inves-\ntigate each agent’s impact in our whole pipeline.\nRetrieval \nAgent\nPlanning \nAgent\nDebugging \nAgent\nPass@1\nPerformance \nDrop\n✗\n✗\n✔\n68.0%\n15.0%\n✗\n✔\n✔\n76.0%\n5.0%\n✗\n✔\n✗\n52.0%\n35.0%\n✔\n✗\n✔\n70.0%\n12.5%\n✔\n✔\n✗\n66.0%\n17.5%\n✔\n✗\n✗\n62.0%\n22.5%\n✔\n✔\n✔\n80.0%\n-\nTable 6:\nPass@1 results for different versions of\nMapCoder{v399}\n6.2\nQualitative Example\nTo verify the above numerical significance, and\nto understand how our method enhance the code\ngeneration, we have performed a qualitative anal-\nysis to find the underlying reason for the superior\nperformance of{v400}\nprompting approaches. An example problem and\nthe output with the explanation of Direct, CoT,\nReflexion, and{v401}\nFigure{v402}. This example demonstrates how the{v403}\n{v404}\na guide from the{v405}. This verifies the\nimpact of these two most significant agents. We\npresent more detailed examples in Appendix.\n6.3\nImpact of{v406}\nMapCoder{v407}\nber of self-retrieved exemplars,{v408}, and the number\nof debugging attempts,{v409}. Our findings (Table{v410})\nreveal that higher{v411},{v412}\ngain at the expense of time.\nDataset Name\n0\n3]",
  "第9章 HumanEval": "[章节内容长度: 144字符。HumanEval\n3\n62.8%\n76.8%\n80.5%\n5\n65.9%\n79.9%\n80.5%\nHumanEval-ET\n3\n57.3%\n61.0%\n70.1%\n5\n57.9%\n67.1%\n67.1%\nTable 7: Pass@1 results by varying{v413}.]",
  "第10章 Average for MapCoder": "[章节内容长度: 1779字符。Average for MapCoder\nAverage for Direct Prompting\nAccuracy \nEnhancement\nLLM\nDataset\nAPI Calls\nTokens (k)\nAPI Calls\nTokens (k)\nChatGPT\nHumanEval\n{v414}\n{v415}\n{v416}\n{v417}\n{v418}\nMBPP\n{v419}\n{v420}\n{v421}\n{v422}\n{v423}\nAPPS\n{v424}\n{v425}\n{v426}\n{v427}\n{v428}\nxCodeEval\n{v429}\n{v430}\n{v431}\n{v432}\n{v433}\nCodeContest\n{v434}\n{v435}\n{v436}\n{v437}\n{v438}\nGPT4\nHumanEval\n{v439}\n{v440}\n{v441}\n{v442}\n{v443}\nMBPP\n{v444}\n{v445}\n{v446}\n{v447}\n{v448}\nAPPS\n{v449}\n{v450}\n{v451}\n{v452}\n{v453}\nxCodeEval\n{v454}\n{v455}\n{v456}\n{v457}\n{v458}\nCodeContest\n{v459}\n{v460}\n{v461}\n{v462}\n{v463}\nAverage\n{v464}\n{v465}\n{v466}\n{v467}\n{v468}\nTable 8: Average number of API calls, thousands of tokens used, required time in minutes to get the API response.\n6.4\nImpact of Number of Sample I/Os\nGiven the limited number of sample I/Os in\nthe HumanEval dataset (average of 2.82 per\nproblem), we supplemented it with an additional\n5 sample I/Os from the HumanEval-ET dataset.\nExperiments with this augmented set showed an\n1.5% performance gain.\n6.5\nError Analysis and Challenges\nAlthough{v469}\nmance compared to other methods, it faces chal-\nlenges in certain algorithmic domains.\nFor ex-\nample, Figure{v470}’s reduced\nperformance on more difficult problems requiring\nprecise problem understanding and concrete plan-\nning—capabilities still lacking in LLMs. In the\nxCodeEval dataset (see Figure{v471}), it solves a lim-\nited number of problems in categories like Combi-\nnatorics, Constructive, Number Theory, Divide and\nConquer, and Dynamic Programming (DP). Man-\nual inspection of five DP category problems reveals\noccasional misinterpretation of problems, attempts\nto solve using greedy or brute-force approaches,\nand struggles with accurate DP table construction\nwhen recognizing the need for a DP solution.]",
  "第11章 Conclusion and Future Work": "[章节内容长度: 742字符。Conclusion and Future Work\nIn this paper, we introduce{v472}, a novel\nframework for effective code generation in complex\nproblem-solving tasks, leveraging the multi-agent\nprompting capabilities of LLMs.{v473}\ntures the complete problem-solving cycle by em-\nploying four agents - retrieval, planning, coding,\nand debugging - which dynamically interact to pro-\nduce high-quality outputs. Evaluation across ma-\njor benchmarks, including basic and competitive\nprogramming datasets, demonstrates{v474}’s\nconsistent outperformance of well-established base-\nlines and SoTA approaches across various metrics.\nFuture work aims to extend this approach to other\ndomains like question answering and mathematical\nreasoning, expanding its scope and impact.]",
  "第12章 Limitations": "[章节内容长度: 1204字符。Limitations\nAmong the limitations of our work,\nfirstly,\nMapCoder{v475}\nwhich may pose challenges in resource-constrained\nenvironments. Table{v476}\nerage API calls and token consumption with the\ndefault{v477}\nperformance) while Table{v478}) shows how{v479},{v480}\nadjusted to proportionate the performance gain at\nthe expense of time/token. We have not addressed\nthe problem of minimizing tokens/API-calls in this\npaper and leave it for future works. Secondly, our\nmethod currently relies on sample input-output\n(I/O) pairs for bug fixing. Although sample I/Os\nprovide valuable insights for LLMs’ code genera-\ntion, their limited number may not always capture\nthe full spectrum of possible test cases. Conse-\nquently, enhancing the quality of additional test\ncase generation could reduce our reliance on sam-\nple I/Os and further improve the robustness of our\napproach. Additionally, future exploration of open-\nsource code generation models, such as CodeL-\nLaMa, LLaMa3, Mixtral 8x7B could offer valu-\nable insights and potential enhancements to our\napproach. Another important concern is that while\nrunning machine-generated code, it is advisable to\nrun it inside a sandbox to avoid any potential risks.]",
  "第13章 References": "[章节内容长度: 11543字符。Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi\nRay, and Kai-Wei Chang. 2021. Unified pre-training\n9\n\n\nfor program understanding and generation.{v481}\n{v482}.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars!{v483}.\nJacob Andreas, John Bufe, David Burkett, Charles\nChen, Josh Clausman, Jean Crawford, Kate Crim,\nJordan DeLoach, Leah Dorner, Jason Eisner, Hao\nFang, Alan Guo, David Hall, Kristin Hayes, Kellie\nHill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan\nKlein, Jayant Krishnamurthy, Theo Lanman, Percy\nLiang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-\nGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij\nPetters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon\nStriplin, Yu Su, Zachary Tellman, Sam Thomson, An-\ndrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby\nWray, Yuchen Zhang, and Alexander Zotov. 2020.\nTask-oriented dialogue as dataflow synthesis.{v484}\n{v485}\n{v486}, 8:556–571.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models.{v487}\n{v488}.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022.\nCodet: Code generation with generated tests.{v489}\n{v490}.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021a.{v491}\ning large language models trained on code.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021b.\nEvaluating large lan-\nguage models trained on code.\n{v492}\n{v493}.\nXinyun Chen, Maxwell Lin, Nathanael Schärli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug.{v494}.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways.{v495}\n{v496}.\nYihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li,\nGe Li, and Zhi Jin. 2023a. Codescore: Evaluating\ncode generation by learning code execution.{v497}\n{v498}.\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023b.\nSelf-collaboration code generation via chatgpt.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. In{v499}\n{v500}, pages 1536–1547.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\n{v501}.\nSumit Gulwani. 2011. Automating string processing\nin spreadsheets using input-output examples.{v502}\n{v503}, 46(1):317–330.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai\nDong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. 2024. Deepseek-coder: When the\nlarge language model meets programming–the rise of\ncode intelligence.{v504}.\nVincent J. Hellendoorn and Premkumar Devanbu. 2017.\nAre deep neural networks the best choice for model-\ning source code?{v505}\n{v506}\n{v507}, ESEC/FSE 2017, pages 763–773, New York,\nNY, USA. ACM.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su,\nand Premkumar Devanbu. 2016.{v508}\nof software.{v509}, 59(5):122–131.\nDong Huang, Qingwen Bu, Jie M Zhang, Michael Luck,\nand Heming Cui. 2023. Agentcoder: Multi-agent-\nbased code generation with iterative testing and opti-\nmisation.{v510}.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timothée Lacroix,\nand William El Sayed. 2023a.{v511}.\n10\n\n\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang,\nand Ge Li. 2023b.\nSelf-planning code genera-\ntion with large language model.\n{v512}\n{v513}.\nMohammad Abdullah Matin Khan, M Saiful Bari,\nXuan Long Do, Weishi Wang, Md Rizwan Parvez,\nand Shafiq Joty. 2023. xcodeeval: A large scale multi-\nlingual multitask benchmark for code understanding,\ngeneration, translation and retrieval.{v514}\n{v515}.\nDonald E Knuth. 1992. Literate programming.{v516}\n{v517}\n{v518}.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning.{v519}\n{v520}, 35:21314–21328.\nJingyao Li, Pengguang Chen, and Jiaya Jia. 2023. Mot-\ncoder: Elevating large language models with modular\nof thought for challenging programming tasks.{v521}\n{v522}.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022a. Competition-level code generation with\nalphacode.{v523}, 378(6624):1092–1097.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, Rémi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\nThomas Hubert, Peter Choy, Cyprien de Mas-\nson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey\nCherepanov, James Molloy, Daniel Mankowitz, Esme\nSutherland Robson, Pushmeet Kohli, Nando de Fre-\nitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022b.\nCompetition-level code generation with alphacode.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling-\nming Zhang. 2023.{v524}\nGPT really correct? rigorous evaluation of large lan-\nguage models for code generation. In{v525}\n{v526}\n{v527}.\nZohar Manna and Richard J. Waldinger. 1971.\nTo-\nward automatic program synthesis.{v528},\n14(3):151–165.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.\n{v529}.\nCarlos Pacheco, Shuvendu K Lahiri, Michael D Ernst,\nand Thomas Ball. 2007. Feedback-directed random\ntest generation. In{v530}\n{v531}, pages 75–84.\nIEEE.\nEmilio Parisotto and Ruslan Salakhutdinov. 2017. Neu-\nral map: Structured memory for deep reinforcement\nlearning.{v532}.\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization.{v533}.\nMd Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray,\nand Kai-Wei Chang. 2018.{v534}\nels for text with named entities. In{v535}\n{v536}\n{v537}, pages\n2373–2383, Melbourne, Australia. Association for\nComputational Linguistics.\nMd Rizwan Parvez, Jianfeng Chi, Wasi Uddin Ahmad,\nYuan Tian, and Kai-Wei Chang. 2023.\nRetrieval\nenhanced data augmentation for question answer-\ning on privacy policies. In{v538}\n{v539}\n{v540}, pages 201–210,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nOleksandr Polozov and Sumit Gulwani. 2015. Flash-\nmeta: A framework for inductive program synthesis.\nIn{v541}\n{v542}\n{v543}, pages 107–\n126.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017.{v544}\nand semantic parsing.{v545}, abs/1704.07535.\nTal Ridnik, Dedy Kredo, and Itamar Friedman. 2024.\nCode generation with alphacodium: From prompt\nengineering to flow engineering.\n{v546}\n{v547}.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code.{v548}\n{v549}.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: Language agents with verbal reinforcement\nlearning. In{v550}\n{v551}.\nKashun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nIn{v552}\n{v553}\n{v554}, pages 12113–12139, Singapore. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\n{v555}\n{v556}.\n11\n\n\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021.\nCodet5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In{v557}, pages 8696–8708.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023. Learning to filter\ncontext for retrieval-augmented generation.{v558}\n{v559}.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022a. Chain-of-thought prompting elicits rea-\nsoning in large language models.{v560}\n{v561}, 35:24824–24837.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022b. Chain-of-thought prompting elicits rea-\nsoning in large language models.{v562}\n{v563}, 35:24824–24837.\nXiaohan Xu, Chongyang Tao, Tao Shen, Can Xu,\nHongbo Xu, Guodong Long, and Jian guang Lou.\n2023.{v564}\nmodels.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts: Deliberate\nproblem solving with large language models.{v565}\n{v566}.\nMichihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong\nPasupat, Jure Leskovec, Percy Liang, Ed H Chi, and\nDenny Zhou. 2023. Large language models as ana-\nlogical reasoners.{v567}.\nPengcheng Yin and Graham Neubig. 2017.{v568}\nneural model for general-purpose code generation.\n{v569}, abs/1704.01696.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019.{v570}\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\n{v571}\n{v572}\n{v573}\n{v574}, pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYifan Zhang, Jingqin Yang, Yang Yuan, and An-\ndrew Chi-Chih Yao. 2023.\nCumulative reason-\ning with large language models.\n{v575}\n{v576}.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompt-\ning in large language models.\n{v577}\n{v578}.\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman,\nHaohan Wang, and Yu-Xiong Wang. 2023.\nLan-\nguage agent tree search unifies reasoning acting\nand planning in language models.{v579}\n{v580}.]",
  "第14章 Appendix": "[章节内容长度: 4553字符。A\nAlgorithm of{v581}\nAlgorithm 1 shows the pseudo-code of our prompt-\ning technique.\nAlgorithm 1{v582}\n1:{v583}number of self-retrieved exemplars\n2:{v584}number of debugging attempts\n3:\n4:{v585}RetrivalAgent({v586})\n5:\n6:{v587}empty array of size{v588}\n7:{v589}\n8:\n{v590}[{v591}]{v592}PlanningAgent({v593})\n9:{v594}\n10:\n11:{v595}SortByConfidence({v596})\n12:\n13:{v597}1{v598}\n14:\n{v599}CodingAgent({v600},{v601}[{v602}])\n15:\n{v603}test({v604},{v605}_{v606})\n16:\nif{v607}\n17:\nReturn{v608}\n18:\nelse\n19:\nfor{v609}1{v610}\n20:\n{v611}DebuggingAgent({v612},{v613})\n21:\n{v614}test({v615},{v616}_{v617})\n22:\nif{v618}\n23:\nReturn{v619}\n24:\nend if\n25:\nend for\n26:\nend if\n27:{v620}\n28: Return{v621}\nB\nDetails Promptings of{v622}\nThe detailed prompting of the Retrieval Agent,\nPlanning Agent, Coding Agent, and Debugging\nAgent are shown in Figure{v623},{v624}, and{v625}\nNote that we adopt a specific sequence of instruc-\ntions in the prompt for Retrieval Agent which is a\ncrucial design choice.\nC\nExample Problem\nTwo complete examples of how{v626}\nby showing all the prompts and responses for all\nfour agents is given in this{v627}.\n12\n\n\n \n \n \n \nGiven a problem, provide relevant problems then identify the algorithm behind it and also explain the tutorial of the algorithm.  \n \n# Problem: \n{Problem Description will be added here} \n \n# Exemplars: \nRecall k relevant and distinct problems (different from problem mentioned above). For each problem,  \n1. describe it \n2. generate {language} code step by step to solve that problem \n3. finally generate a planning to solve that problem \n \n# Algorithm: \n---------------- \nImportant: \nYour response must follow the following xml format- \n{v628}\n{v629}\n# Recall k relevant and distinct problems (different from problem mentioned above). Write each problem in the following format. \n{v630} \n{v631} \n{v632}# Planning to solve this problem. {v633} \n{v634}\n# similarly add more problems here... \n{v635}\n# Identify the algorithm (Brute-force, Dynamic Programming, Divide-and-conquer, Greedy, Backtracking, Recursive, Binary search, \nand so on) that needs to be used to solve the original problem. \n# Write a useful tutorial about the above mentioned algorithms. Provide a high level generic tutorial for solving this types \nof problem. Do not generate code. \n{v636}\n{v637}\nRetrieval Agent \nFigure 8: Prompt for self-retrieval Agent.\n \n \n \n \nPlanning Generation Prompt: \nGiven a competitive programming problem \ngenerate a concrete planning to solve the \nproblem. \n# Problem: {Description of the example problem} \n# Planning: {Planning of the example problem} \n## Relevant Algorithm to solve the next \nproblem: \n{Algorithm retrieved by Retrieval Agent} \n## Problem to be solved: {Original Problem} \n## Sample Input/Outputs: {Sample IOs} \n \n---------------- \nImportant:{v638}\nsolve the problem. Do not add extra explanation \nor words. \n{v639}\nGiven a competitive programming problem and a plan to solve \nthe problem in {language} tell whether the plan is correct to \nsolve this problem. \n \n# Problem:  {Original Problem} \n# Planning: {Planning of our problem from previous step} \n \n---------------- \nImportant:{v640}\n{v641}\n{v642}\nprogramming problem is solvable by using the above mentioned \nplanning. {v643} \n{v644}\nthe problem. Must be an integer between 0 and 100. \n{v645} \n{v646} \n \nPlanning Agent \nFigure 9: Prompt for Planning Agent. The example problems that are mentioned in this figure will come from the\nRetrieval Agent.\n \n \n \nGiven a competitive programming problem generate \nPython3 code to solve the problem. \n \n## Relevant Algorithm to solve the next problem:{v647}\n{Algorithm retrieved by Retrieval Agent} \n## Problem to be solved: \n{Our Problem Description will be added here} \n## Planning:{v648}\n## Sample Input/Outputs: {Sample I/Os} \n## Let's think step by step. \n------------ \nImportant: \n## Your response must contain only the {language} code \nto solve this problem. Do not add extra explanation or \nwords. \nCoding Agent \n \nGiven a competitive programming problem you have generated {language} \ncode to solve the problem. But the generated code cannot pass sample \ntest cases. Improve your code to solve the problem correctly. \n \n## Relevant Algorithm to solve the next problem:{v649}\n{Algorithm retrieved by Retrieval Agent} \n## Planning:{v650}\n## Code:{v651}\n## Modified Planning: \n## Let's think step by step{v652}\nthis problem. \n \n---------------- \nImportant: \n## Your response must contain the modified planning and then the \n{v653}\nDebugging Agent \nFigure 10: Prompt for Coding and Debugging Agent.\n13]"
}